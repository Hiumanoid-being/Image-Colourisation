{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14025258,"sourceType":"datasetVersion","datasetId":8931748}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Library Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.parallel import DataParallel\nfrom torch.cuda.amp import autocast, GradScaler\nfrom pathlib import Path\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nimport torchvision.transforms.functional as TF\nimport random\nfrom skimage import color","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Architecture\nclass CNNEncoder(nn.Module):\n    def __init__(self, in_channels=1, feature_dim=256):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256, feature_dim, 3, stride=1, padding=1),\n            nn.BatchNorm2d(feature_dim),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x1 = self.conv1(x)  # [B, 64, H, W]\n        x2 = self.conv2(x1) # [B, 128, H/2, W/2]\n        x3 = self.conv3(x2) # [B, 256, H/4, W/4]\n        x4 = self.conv4(x3) # [B, 256, H/4, W/4]\n        return x4, (x1, x2, x3)\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, feature_dim=256, num_heads=8, num_layers=4, patch_size=4, dropout=0.1):\n        super().__init__()\n        self.feature_dim = feature_dim\n        self.patch_size = patch_size\n        \n        patch_dim = patch_size * patch_size * feature_dim\n        self.patch_projection = nn.Linear(patch_dim, feature_dim)\n        self.pos_embed = nn.Parameter(torch.randn(1, 256, feature_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=feature_dim,\n            nhead=num_heads,\n            dim_feedforward=feature_dim * 4,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        # Split feature map into patches\n        x = x.unfold(2, self.patch_size, self.patch_size)\\\n             .unfold(3, self.patch_size, self.patch_size)\n        x = x.contiguous().view(B, C, -1, self.patch_size * self.patch_size)\n        x = x.permute(0, 2, 1, 3).flatten(2)\n\n        # Project patches\n        x = self.patch_projection(x)\n        \n        # Add positional embedding\n        num_patches = x.size(1)\n        if num_patches <= self.pos_embed.size(1):\n            pos_emb = self.pos_embed[:, :num_patches, :]\n        else:\n            pos_emb = F.interpolate(\n                self.pos_embed.permute(0, 2, 1),\n                size=num_patches,\n                mode='linear',\n                align_corners=False\n            ).permute(0, 2, 1)\n        \n        x = x + pos_emb\n        x = self.transformer(x)\n        return x, (H, W)\n\n\nclass ColorDecoder(nn.Module):\n    def __init__(self, feature_dim=256):\n        super().__init__()\n        self.up1 = nn.Sequential(\n            nn.ConvTranspose2d(feature_dim, 256, 4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n        \n        self.up2 = nn.Sequential(\n            nn.ConvTranspose2d(256 + 256, 128, 4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n        )\n        \n        self.up3 = nn.Sequential(\n            nn.Conv2d(128 + 128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n        )\n        \n        self.final = nn.Sequential(\n            nn.Conv2d(128 + 64, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 2, 3, stride=1, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, orig_H, orig_W, skip_features=None):\n        B, N, C = x.shape\n        side = int(N ** 0.5)\n        \n        x = x.permute(0, 2, 1).view(B, C, side, side)\n        x = self.up1(x)\n        \n        if skip_features is not None:\n            skip3 = skip_features[2]\n            if skip3.shape[2:] != x.shape[2:]:\n                skip3 = F.interpolate(skip3, size=x.shape[2:], mode='bilinear', align_corners=False)\n            x = torch.cat([x, skip3], dim=1)\n        \n        x = self.up2(x)\n        \n        if skip_features is not None:\n            skip2 = skip_features[1]\n            if skip2.shape[2:] != x.shape[2:]:\n                skip2 = F.interpolate(skip2, size=x.shape[2:], mode='bilinear', align_corners=False)\n            x = torch.cat([x, skip2], dim=1)\n        \n        x = self.up3(x)\n        \n        if skip_features is not None:\n            skip1 = skip_features[0]\n            if skip1.shape[2:] != x.shape[2:]:\n                skip1 = F.interpolate(skip1, size=x.shape[2:], mode='bilinear', align_corners=False)\n            x = torch.cat([x, skip1], dim=1)\n        \n        x = self.final(x)\n        \n        if x.shape[2] != orig_H or x.shape[3] != orig_W:\n            x = F.interpolate(x, size=(orig_H, orig_W), mode=\"bilinear\", align_corners=False)\n        \n        return x\n\n\nclass CNNTransformerColorizer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = CNNEncoder()\n        self.transformer = TransformerEncoder(dropout=0.1)\n        self.decoder = ColorDecoder()\n\n    def forward(self, x):\n        orig_H, orig_W = x.shape[2], x.shape[3]\n        feat, skip_features = self.encoder(x)\n        trans, (enc_H, enc_W) = self.transformer(feat)\n        ab = self.decoder(trans, orig_H, orig_W, skip_features)\n        return ab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss Function\nclass ImprovedColorizationLoss(nn.Module):\n    def __init__(self, l1_weight=1.0, perceptual_weight=0.1, saturation_weight=0.05):\n        \"\"\"\n        Balanced loss function with all components on similar scales\n        \n        Arguements:\n            l1_weight: Weight for L1 loss (main pixel-wise accuracy)\n            perceptual_weight: Weight for MSE loss (perceptual color matching)\n            saturation_weight: Weight for saturation matching\n        \"\"\"\n        super().__init__()\n        self.l1_weight = l1_weight\n        self.perceptual_weight = perceptual_weight\n        self.saturation_weight = saturation_weight\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        \n    def perceptual_color_loss(self, pred, target):\n        # MSE loss on AB channels - much more stable than histogram matching\n        return self.mse_loss(pred, target)\n    \n    def saturation_loss(self, pred, target):\n        # Larger epsilon to prevent NaN from sqrt of negative numbers\n        pred_sat = torch.sqrt(torch.clamp(pred[:, 0:1]**2 + pred[:, 1:2]**2, min=0) + 1e-6)\n        target_sat = torch.sqrt(torch.clamp(target[:, 0:1]**2 + target[:, 1:2]**2, min=0) + 1e-6)\n        return self.mse_loss(pred_sat, target_sat)\n    \n    def forward(self, pred, target):\n        # L1 loss for overall color accuracy\n        l1 = self.l1_loss(pred, target)\n        \n        # Perceptual loss (MSE) for color matching\n        perceptual = self.perceptual_color_loss(pred, target)\n        \n        # Saturation matching\n        sat = self.saturation_loss(pred, target)\n        \n        total_loss = (self.l1_weight * l1 + \n                     self.perceptual_weight * perceptual + \n                     self.saturation_weight * sat)\n        \n        return total_loss, {\n            'l1': l1.item(), \n            'perceptual': perceptual.item(), \n            'saturation': sat.item()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataloader for batch processing\nclass ColorizationDataset(Dataset):\n    def __init__(self, data_dir, split=\"train\", max_samples=100000, augment=True):\n        data_dir = Path(data_dir)\n        g_root = data_dir / \"grayscale\" / split\n        c_root = data_dir / \"color\" / split\n\n        if not g_root.exists() or not c_root.exists():\n            raise RuntimeError(\n                f\"Expected folders:\\n{g_root}\\n{c_root}\\nbut they were not found.\"\n            )\n\n        print(f\"ðŸ” Loading {split.upper()} split\")\n        print(f\"Searching for grayscale images in: {g_root}\")\n\n        self.gray_files = []\n        for root, dirs, files in os.walk(g_root):\n            jpg_files = [f for f in files if f.lower().endswith(\".jpg\")]\n            for f in jpg_files:\n                self.gray_files.append(Path(root) / f)\n                if len(self.gray_files) >= max_samples:\n                    break\n            if len(self.gray_files) >= max_samples:\n                break\n\n        print(f\"  Total grayscale images found: {len(self.gray_files)}\")\n\n        self.gray_files = sorted(self.gray_files)\n        self.gray_root = g_root\n        self.color_root = c_root\n        self.augment = augment and (split == \"train\")\n\n    def __len__(self):\n        return len(self.gray_files)\n\n    def __getitem__(self, idx):\n        gray_path = self.gray_files[idx]\n        rel = gray_path.relative_to(self.gray_root)\n        color_path = (self.color_root / rel).with_suffix(\".npy\")\n\n        # Load data\n        L = np.array(Image.open(gray_path)).astype(\"float32\")\n        L = (L / 127.5) - 1.0\n        AB = np.load(color_path).astype(\"float32\")\n\n        L = torch.tensor(L).unsqueeze(0)\n        AB = torch.tensor(AB).permute(2, 0, 1)\n\n        # Apply augmentation\n        if self.augment:\n            # Random horizontal flip\n            if random.random() > 0.5:\n                L = TF.hflip(L)\n                AB = TF.hflip(AB)\n            \n            # Random vertical flip\n            if random.random() > 0.5:\n                L = TF.vflip(L)\n                AB = TF.vflip(AB)\n            \n            # Random rotation (small angles)\n            if random.random() > 0.7:\n                angle = random.uniform(-15, 15)\n                L = TF.rotate(L, angle)\n                AB = TF.rotate(AB, angle)\n\n        return L, AB","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Script to fit Kaggle use case\ndef train_with_validation_amp(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=100, save_dir=\"/kaggle/working\", \n                              patience=20, start_epoch=0, resume_loss_history=None):\n    model.train()\n    save_path = Path(save_dir)\n    save_path.mkdir(exist_ok=True)\n    \n    # Gradient scaler for mixed precision\n    scaler = GradScaler()\n    \n    # Initialize or resume loss history\n    if resume_loss_history is not None:\n        loss_history = resume_loss_history\n        print(f\" Resuming with {len(loss_history['train_total'])} previous epochs of history\")\n    else:\n        loss_history = {\n            'train_total': [], 'train_l1': [], 'train_perceptual': [], 'train_saturation': [],\n            'val_total': [], 'val_l1': [], 'val_perceptual': [], 'val_saturation': [],\n            'lr': []\n        }\n    \n    # Find best validation loss from history\n    if len(loss_history['val_total']) > 0:\n        best_val_loss = min(loss_history['val_total'])\n        best_epoch = loss_history['val_total'].index(best_val_loss) + 1\n        print(f\" Previous best validation loss: {best_val_loss:.6f} at epoch {best_epoch}\")\n    else:\n        best_val_loss = float('inf')\n    \n    patience_counter = 0\n    \n    for epoch in range(start_epoch, epochs):\n        # Training pahse\n        model.train()\n        train_loss = 0.0\n        train_l1 = 0.0\n        train_perceptual = 0.0\n        train_sat = 0.0\n        \n        pbar = tqdm(enumerate(train_loader), total=len(train_loader), \n                    desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        \n        for i, (L, AB) in pbar:\n            L, AB = L.to(device, non_blocking=True), AB.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            \n            # Mixed precision forward pass\n            with autocast():\n                output = model(L)\n                \n                if output.shape != AB.shape:\n                    output = F.interpolate(output, size=(AB.shape[2], AB.shape[3]), mode='bilinear', align_corners=False)\n                \n                loss, loss_components = criterion(output, AB)\n            \n            # Mixed precision backward pass with NaN detection\n            scaler.scale(loss).backward()\n            \n            # Check for NaN gradients before stepping\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"  NaN/Inf loss detected at batch {i}, skipping...\")\n                optimizer.zero_grad()\n                continue\n            \n            scaler.unscale_(optimizer)\n            \n            # More aggressive gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n            \n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_loss += loss.item()\n            train_l1 += loss_components['l1']\n            train_perceptual += loss_components['perceptual']\n            train_sat += loss_components['saturation']\n            \n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = train_loss / len(train_loader)\n        avg_train_l1 = train_l1 / len(train_loader)\n        avg_train_perceptual = train_perceptual / len(train_loader)\n        avg_train_sat = train_sat / len(train_loader)\n        \n        # Validation Phase\n        model.eval()\n        val_loss = 0.0\n        val_l1 = 0.0\n        val_perceptual = 0.0\n        val_sat = 0.0\n        \n        with torch.no_grad():\n            for L, AB in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False):\n                L, AB = L.to(device, non_blocking=True), AB.to(device, non_blocking=True)\n                \n                # Autocast for validation \n                with autocast():\n                    output = model(L)\n                    \n                    if output.shape != AB.shape:\n                        output = F.interpolate(output, size=(AB.shape[2], AB.shape[3]), mode='bilinear', align_corners=False)\n                    \n                    loss, loss_components = criterion(output, AB)\n                \n                val_loss += loss.item()\n                val_l1 += loss_components['l1']\n                val_perceptual += loss_components['perceptual']\n                val_sat += loss_components['saturation']\n        \n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_l1 = val_l1 / len(val_loader)\n        avg_val_perceptual = val_perceptual / len(val_loader)\n        avg_val_sat = val_sat / len(val_loader)\n        \n        # Step scheduler\n        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n            current_lr = optimizer.param_groups[0]['lr']\n        elif scheduler is not None:\n            scheduler.step()\n            current_lr = scheduler.get_last_lr()[0]\n        else:\n            current_lr = optimizer.param_groups[0]['lr']\n        \n        # Store history\n        loss_history['train_total'].append(avg_train_loss)\n        loss_history['train_l1'].append(avg_train_l1)\n        loss_history['train_perceptual'].append(avg_train_perceptual)\n        loss_history['train_saturation'].append(avg_train_sat)\n        loss_history['val_total'].append(avg_val_loss)\n        loss_history['val_l1'].append(avg_val_l1)\n        loss_history['val_perceptual'].append(avg_val_perceptual)\n        loss_history['val_saturation'].append(avg_val_sat)\n        loss_history['lr'].append(current_lr)\n        \n        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n        print(f\"Train Loss: {avg_train_loss:.6f} (L1: {avg_train_l1:.6f}, Perceptual: {avg_train_perceptual:.6f}, Sat: {avg_train_sat:.6f})\")\n        print(f\"Val Loss:   {avg_val_loss:.6f} (L1: {avg_val_l1:.6f}, Perceptual: {avg_val_perceptual:.6f}, Sat: {avg_val_sat:.6f})\")\n        print(f\"LR: {current_lr:.2e}\")\n        \n        # Early stopping\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            \n            if hasattr(model, 'module'):\n                model_to_save = model.module\n            else:\n                model_to_save = model\n            \n            best_model_path = save_path / \"best_model.pth\"\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model_to_save.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': best_val_loss,\n                'loss_history': loss_history,\n            }, best_model_path)\n            print(f\" New best model saved;; Val loss: {best_val_loss:.6f}\")\n        else:\n            patience_counter += 1\n            print(f\"  No improvement for {patience_counter}/{patience} epochs\")\n            \n            if patience_counter >= patience:\n                print(f\"\\n  Early stopping triggered after {epoch+1} epochs\")\n                print(f\"  Best validation loss: {best_val_loss:.6f}\")\n                break\n        \n        # Create checkpoints incase of deathly crashes\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"\\n  Checkpoint at epoch {epoch+1}\")\n            \n            if hasattr(model, 'module'):\n                model_to_save = model.module\n            else:\n                model_to_save = model\n            \n            checkpoint_path = save_path / f\"checkpoint_epoch_{epoch+1:03d}.pth\"\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model_to_save.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,\n                'loss_history': loss_history,\n                'val_loss': avg_val_loss,\n            }, checkpoint_path)\n            print(f\"  Checkpoint saved: {checkpoint_path}\")\n            \n            try:\n                visualization_path = save_path / f\"visualization_epoch_{epoch+1:03d}.png\"\n                generate_visualization(model_to_save, val_loader.dataset, device, save_path=visualization_path, epoch=epoch+1, saturation_boost=1.6)\n                print(f\"  Visualization saved\")\n            except Exception as e:\n                print(f\"  Visualization failed: {e}\")\n            \n            plot_loss_history_with_validation(loss_history, save_path / f\"loss_plot_epoch_{epoch+1:03d}.png\")\n            print(f\"  Loss plot saved\")\n        \n        # Save checkpoint every 5 epochs incase of death ly crash\n        elif (epoch + 1) % 5 == 0:\n            if hasattr(model, 'module'):\n                model_to_save = model.module\n            else:\n                model_to_save = model\n            \n            checkpoint_path = save_path / f\"checkpoint_epoch_{epoch+1:03d}.pth\"\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model_to_save.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,\n                'loss_history': loss_history,\n                'val_loss': avg_val_loss,\n            }, checkpoint_path)\n            print(f\" Quick checkpoint saved: {checkpoint_path}\")\n    \n    print(\"\\n If you see this, it has been more than 12 hours\")\n    return model, loss_history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualisation\ndef generate_visualization(model, dataset, device, num_samples=3, saturation_boost=1.5, save_path=None, epoch=None):\n    # Visualize model predictions with higher saturation boost\n\n    model.eval()\n    idxs = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n    \n    fig = plt.figure(figsize=(15, 4 * len(idxs)))\n    \n    if epoch is not None:\n        plt.suptitle(f\"Epoch {epoch} - Model Predictions\", fontsize=16, y=0.995)\n    \n    for i, idx in enumerate(idxs):\n        L, AB = dataset[idx]\n        \n        # Get prediction\n        with torch.no_grad():\n            pred_AB = model(L.unsqueeze(0).to(device)).cpu().squeeze(0)\n        \n        # Apply saturation boost\n        pred_AB_boosted = pred_AB * saturation_boost\n        pred_AB_boosted = torch.clamp(pred_AB_boosted, -1.0, 1.0)\n        \n        # Convert to numpy\n        L_np = L.squeeze().numpy()\n        pred_AB_np = pred_AB.permute(1, 2, 0).numpy()\n        pred_AB_boosted_np = pred_AB_boosted.permute(1, 2, 0).numpy()\n        AB_gt_np = AB.permute(1, 2, 0).numpy()\n        \n        # Convert back to CIELAB range\n        L_lab = (L_np + 1.0) * 50.0\n        pred_AB_lab = pred_AB_np * 128.0\n        pred_AB_boosted_lab = pred_AB_boosted_np * 128.0\n        AB_gt_lab = AB_gt_np * 128.0\n        \n        # Reconstruct LAB images\n        pred_lab = np.zeros((L_lab.shape[0], L_lab.shape[1], 3))\n        pred_lab[:, :, 0] = L_lab\n        pred_lab[:, :, 1:] = pred_AB_lab\n        \n        pred_boosted_lab = np.zeros_like(pred_lab)\n        pred_boosted_lab[:, :, 0] = L_lab\n        pred_boosted_lab[:, :, 1:] = pred_AB_boosted_lab\n        \n        gt_lab = np.zeros_like(pred_lab)\n        gt_lab[:, :, 0] = L_lab\n        gt_lab[:, :, 1:] = AB_gt_lab\n        \n        # Convert LAB to RGB\n        pred_rgb = color.lab2rgb(pred_lab)\n        pred_boosted_rgb = color.lab2rgb(pred_boosted_lab)\n        gt_rgb = color.lab2rgb(gt_lab)\n        \n        # Plot\n        plt.subplot(len(idxs), 4, 4 * i + 1)\n        plt.imshow(L_np, cmap=\"gray\", vmin=-1, vmax=1)\n        plt.title(\"Input Grayscale\")\n        plt.axis(\"off\")\n\n        plt.subplot(len(idxs), 4, 4 * i + 2)\n        plt.imshow(np.clip(pred_rgb, 0, 1))\n        plt.title(\"Predicted Colour\")\n        plt.axis(\"off\")\n        \n        plt.subplot(len(idxs), 4, 4 * i + 3)\n        plt.imshow(np.clip(pred_boosted_rgb, 0, 1))\n        plt.title(f\"Boosted (x{saturation_boost})\")\n        plt.axis(\"off\")\n\n        plt.subplot(len(idxs), 4, 4 * i + 4)\n        plt.imshow(np.clip(gt_rgb, 0, 1))\n        plt.title(\"Ground Truth\")\n        plt.axis(\"off\")\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        plt.close(fig)\n    else:\n        plt.show()\n\n\ndef plot_loss_history_with_validation(loss_history, save_path=None):\n    #Plot training and validation losses\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    epochs = range(1, len(loss_history['train_total']) + 1)\n    \n    # Total loss\n    ax1.plot(epochs, loss_history['train_total'], label='Train', linewidth=2)\n    ax1.plot(epochs, loss_history['val_total'], label='Validation', linewidth=2)\n    ax1.set_title('Total Loss', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # L1 loss\n    ax2.plot(epochs, loss_history['train_l1'], label='Train', linewidth=2)\n    ax2.plot(epochs, loss_history['val_l1'], label='Validation', linewidth=2)\n    ax2.set_title('L1 Loss', fontsize=14, fontweight='bold')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('L1 Loss')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Perceptual loss\n    ax3.plot(epochs, loss_history['train_perceptual'], label='Train', linewidth=2)\n    ax3.plot(epochs, loss_history['val_perceptual'], label='Validation', linewidth=2)\n    ax3.set_title('Perceptual Loss (MSE)', fontsize=14, fontweight='bold')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Perceptual Loss')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Learning rate\n    ax4.plot(epochs, loss_history['lr'], linewidth=2, color='green')\n    ax4.set_title('Learning Rate', fontsize=14, fontweight='bold')\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Learning Rate')\n    ax4.set_yscale('log')\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        plt.close(fig)\n    else:\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resume Training due to 12hr limit\ndef find_checkpoint_in_input():\n    input_dir = Path(\"/kaggle/input\")\n    \n    if not input_dir.exists():\n        print(\"No /kaggle/input directory found\")\n        return None\n    \n    checkpoint_files = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith('.pth'):\n                full_path = os.path.join(root, file)\n                checkpoint_files.append(full_path)\n                print(f\"  Found: {full_path}\")\n    \n    if len(checkpoint_files) == 0:\n        print(\"  No .pth files found\")\n        return None\n    elif len(checkpoint_files) == 1:\n        print(f\"\\n Using checkpoint: {checkpoint_files[0]}\")\n        return checkpoint_files[0]\n    else:\n        print(f\"\\n  Found {len(checkpoint_files)} checkpoint files. Which is: {checkpoint_files[0]}\")\n        print(\"   If you want a different one, specify it manually in main(resume_from=...) !!!\")\n        return checkpoint_files[0]\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):\n    \"\"\"\n    Load model checkpoint and optionally resume training state\n    \n    Args:\n        checkpoint_path: Path to .pth checkpoint file\n        model: Model to load weights into\n        optimizer: Optional optimizer to restore state\n        scheduler: Optional scheduler to restore state\n    \n    Returns:\n        start_epoch: Epoch to resume from\n        loss_history: Previous training history\n    \"\"\"\n    print(f\"  Loading checkpoint from: {checkpoint_path}\")\n    \n    # Check if file exists\n    if not os.path.exists(checkpoint_path):\n        raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n    \n    checkpoint = torch.load(checkpoint_path)\n    \n    # Load model weights with compatibility for DataParallel\n    state_dict = checkpoint['model_state_dict']\n    \n    # Handle DataParallel compatibility\n    # If saved with DataParallel but loading without, or vice versa\n    try:\n        if hasattr(model, 'module'):\n            # Current model has DataParallel wrapper\n            try:\n                model.module.load_state_dict(state_dict)\n            except:\n                # Checkpoint might not have 'module.' prefix, add it\n                new_state_dict = {'module.' + k: v for k, v in state_dict.items()}\n                model.load_state_dict(new_state_dict)\n        else:\n            # Current model doesn't have DataParallel wrapper\n            try:\n                model.load_state_dict(state_dict)\n            except:\n                # Checkpoint has 'module.' prefix, remove it\n                new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n                model.load_state_dict(new_state_dict)\n        print(f\"  Model weights loaded from epoch {checkpoint['epoch']}\")\n    except Exception as e:\n        print(f\"  Error loading model weights: {e}\")\n        print(\"Trying alternative loading method...\")\n        # Last resort - try both ways\n        if hasattr(model, 'module'):\n            new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n            model.module.load_state_dict(new_state_dict)\n        else:\n            new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n            model.load_state_dict(new_state_dict)\n        print(f\"  Model weights loaded successfully using fallback method\")\n    \n    # Load optimizer state\n    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n        try:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            print(f\"  Optimizer state restored\")\n        except Exception as e:\n            print(f\"  Warning: Could not restore optimizer state: {e}\")\n            print(\"   Continuing with fresh optimizer state...\")\n    \n    # Load scheduler state\n    if scheduler is not None and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None:\n        try:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            print(f\"  Scheduler state restored\")\n        except Exception as e:\n            print(f\"  Warning: Could not restore scheduler state: {e}\")\n            print(\"   Continuing with fresh scheduler state...\")\n    \n    # Get training info\n    start_epoch = checkpoint.get('epoch', 0)\n    loss_history = checkpoint.get('loss_history', None)\n    \n    print(f\"  Resuming from epoch {start_epoch}\")\n    if 'val_loss' in checkpoint:\n        print(f\"  Previous validation loss at this checkpoint: {checkpoint['val_loss']:.6f}\")\n    \n    return start_epoch, loss_history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset Loader\ndef find_dataset_path():\n    \"\"\"Automatically find the dataset path in Kaggle\"\"\"\n    input_dir = Path(\"/kaggle/input/150k-data\")\n    \n    if not input_dir.exists():\n        print(\"/kaggle/input/150k-data directory not found\")\n        return None\n    \n    available_datasets = [d for d in input_dir.iterdir() if d.is_dir()]\n    print(f\"Available datasets: {[d.name for d in available_datasets]}\")\n    \n    # Look for processed data\n    for dataset in available_datasets:\n        potential_path = dataset / \"processed\"\n        if potential_path.exists():\n            print(f\"Found processed data at: {potential_path}\")\n            return str(potential_path)\n    \n    # Look for train/grayscale structure directly\n    for dataset in available_datasets:\n        potential_train = dataset / \"grayscale\" / \"train\"\n        if potential_train.exists():\n            print(f\"Found data at: {dataset}\")\n            return str(dataset)\n    \n    # Use the first dataset as fallback\n    if available_datasets:\n        fallback = available_datasets[0]\n        print(f\"Using fallback dataset: {fallback}\")\n        return str(fallback)\n    \n    print(\"No datasets found\")\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Training Loop\ndef main(resume_from=\"auto\"):\n    \"\"\"\n    Main training function\n    \n    Args:\n        resume_from: Path to checkpoint to resume from\n                     - \"auto\": Automatically search in /kaggle/input\n                     - None or False: Start fresh training\n                     - \"/path/to/checkpoint.pth\": Load specific checkpoint\n    \"\"\"\n    # Auto-detect checkpoint if requested\n    if resume_from == \"auto\":\n        resume_from = find_checkpoint_in_input()\n        if resume_from is None:\n            print(\"  No checkpoint found, starting fresh training, let wait!!!\")\n    elif resume_from == False or resume_from == \"\":\n        resume_from = None\n    \n    # GPU configuration\n    num_gpus = torch.cuda.device_count()\n    print(f\" Found {num_gpus} GPU(s)\")\n    \n    for i in range(num_gpus):\n        gpu_props = torch.cuda.get_device_properties(i)\n        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"    Memory: {gpu_props.total_memory / 1024**3:.1f} GB\")\n    \n    # Setup device and model\n    if num_gpus > 1:\n        device = torch.device(\"cuda:0\")\n        print(f\"Using {num_gpus} GPUs with DataParallel\")\n        model = CNNTransformerColorizer()\n        model = DataParallel(model)\n        model = model.to(device)\n        batch_size = 8 * num_gpus\n    else:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model = CNNTransformerColorizer().to(device)\n        batch_size = 8\n        print(f\"Using device: {device}\")\n\n    # Auto-detect dataset path\n    data_dir = find_dataset_path()\n    if data_dir is None:\n        print(\" No dataset found :(\")\n        return\n    \n    print(f\"\\n Using data directory: {data_dir}\\n\")\n    \n    # Load datasets with augmentation\n    try:\n        # Set ur training data and validation data here\n        train_dataset = ColorizationDataset(data_dir, split=\"train\", max_samples=200000, augment=True)\n        val_dataset = ColorizationDataset(data_dir, split=\"val\", max_samples=4000, augment=False)\n        \n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=batch_size, \n            shuffle=True, \n            num_workers=2,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset, \n            batch_size=batch_size, \n            shuffle=False, \n            num_workers=2,\n            pin_memory=True\n        )\n        \n        print(f\" Training with {len(train_dataset)} images\")\n        print(f\" Validating with {len(val_dataset)} images\")\n        print(f\" Batch size: {batch_size}\")\n        \n    except Exception as e:\n        print(f\" Error loading dataset: {e}\")\n        print(\"\\nAvailable directories:\")\n        for root, dirs, files in os.walk(data_dir):\n            level = root.replace(str(data_dir), '').count(os.sep)\n            if level < 3:  # Limit depth\n                indent = ' ' * 2 * level\n                print(f'{indent}{os.path.basename(root)}/')\n        return\n\n    # Setup training components\n    print(\"\\n  Configuring training...\")\n    \n    # Criterion\n    criterion = ImprovedColorizationLoss(\n        l1_weight=0.8,\n        perceptual_weight=0.1,\n        saturation_weight=0.15 \n    )\n    \n    # Adam Optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(), \n        lr=8e-4,  \n        weight_decay=1e-4,\n        eps=1e-7\n    )\n    \n    # Learning rate scheduler \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5,\n        patience=12,  \n        verbose=True, \n        min_lr=1e-6\n    )\n    \n    print(\" Training starting soon....\")\n    start_epoch = 0\n    resume_loss_history = None\n    \n    if resume_from is not None:\n        print(\" RESUMING FROM CHECKPOINT\")\n        start_epoch, resume_loss_history = load_checkpoint(\n            resume_from, model, optimizer, scheduler\n        )\n    \n    if resume_from:\n        print(f\" RESUMING TRAINING FROM EPOCH {start_epoch}\")\n        print(\" Using new weights\")\n    else:\n        print(\" Starting new model from scratch\")\n\n    \n    # Final Training Parameters\n    model, loss_history = train_with_validation_amp(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        epochs=150,\n        save_dir=\"/kaggle/working\",\n        patience=25,\n        start_epoch=start_epoch,\n        resume_loss_history=resume_loss_history\n    )\n\n    print(\"\\n\")\n    print(\" Congrats on waiting, training is completed\")\n    print(f\" All outputs saved to: /kaggle/working/\")\n    print(\"\\nSaved files:\")\n    print(\"  - best_model.pth - Best model based on validation loss\")\n    print(\"  - checkpoint_epoch_XXX.pth - Periodic checkpoints\")\n    print(\"  - visualization_epoch_XXX.png - Sample predictions\")\n    print(\"  - loss_plot_epoch_XXX.png - Training curves\")\n    \n    # Generate final visualization on best model\n    print(\"\\n Generating final visualization from best model...\")\n    try:\n        best_checkpoint = torch.load(\"/kaggle/working/best_model.pth\")\n        if hasattr(model, 'module'):\n            model.module.load_state_dict(best_checkpoint['model_state_dict'])\n        else:\n            model.load_state_dict(best_checkpoint['model_state_dict'])\n        \n        generate_visualization(\n            model.module if hasattr(model, 'module') else model,\n            val_dataset, \n            device, \n            num_samples=5,\n            saturation_boost=1.6,\n            save_path=\"/kaggle/working/final_visualization.png\"\n        )\n        print(\" Final visualization saved: final_visualization.png\")\n        \n        # Generate one with higher saturation for comparison\n        generate_visualization(\n            model.module if hasattr(model, 'module') else model,\n            val_dataset, \n            device, \n            num_samples=5,\n            saturation_boost=2.2, \n            save_path=\"/kaggle/working/final_visualization_boosted.png\"\n        )\n        print(\" High saturation visualization saved: final_visualization_boosted.png\")\n        \n    except Exception as e:\n        print(f\" Error: {e}\")\n    \n    # Print final statistics\n    print(\"\\n Training Statistics:\")\n    print(f\"   Total epochs trained: {len(loss_history['train_total'])}\")\n    print(f\"   Best validation loss: {min(loss_history['val_total']):.6f}\")\n    print(f\"   Final learning rate: {loss_history['lr'][-1]:.2e}\")\n    \n    print(\"\\n Check ur working directory at the side bar. Click refresh if it does not show up\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run \nif __name__ == \"__main__\":\n    # Start new training\n    main(resume_from=None)\n    \n    # Resume from checkpoint (copy path and paste it here)\n    # main(resume_from=\"/kaggle/input/your-checkpoint/checkpoint.pth\")\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}